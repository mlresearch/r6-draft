---
abstract: In this paper, we develop the continuous time dynamic topic model (cDTM).
  The cDTM is a dynamic topic model that uses Brownian motion to model the latent
  topics through a sequential collection of documents, where a "topic" is a pattern
  of word use that we expect to evolve over the course of the collection. We derive
  an efficient variational approximate inference algorithm that takes advantage of
  the sparsity of observations in text, a property that lets us easily handle many
  time points. In contrast to the cDTM, the original discrete-time dynamic topic model
  (dDTM) requires that time be discretized. Moreover, the complexity of variational
  inference for the dDTM grows quickly as time granularity increases, a drawback which
  limits fine-grained discretization. We demonstrate the cDTM on two news corpora,
  reporting both predictive perplexity and the novel task of time stamp prediction.
title: Continuous time dynamic topic models
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang08a
month: 0
tex_title: Continuous time dynamic topic models
firstpage: 579
lastpage: 586
page: 579-586
order: 579
cycles: false
bibtex_author: Wang, Chong and Blei, David and Heckerman, David
author:
- given: Chong
  family: Wang
- given: David
  family: Blei
- given: David
  family: Heckerman
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/wang08a/wang08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
