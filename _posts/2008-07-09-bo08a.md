---
abstract: We propose a variable decomposition algorithm-greedy block coordinate descent
  (GBCD)-in order to make dense Gaussian process regression practical for large scale
  problems. GBCD breaks a large scale optimization into a series of small sub-problems.
  The challenge in variable decomposition algorithms is the identification of a sub-problem
  (the active set of variables) that yields the largest improvement. We analyze the
  limitations of existing methods and cast the active set selection into a zero-norm
  constrained optimization problem that we solve using greedy methods. By directly
  estimating the decrease in the objective function, we obtain not only efficient
  approximate solutions for GBCD, but we are also able to demonstrate that the method
  is globally convergent. Empirical comparisons against competing dense methods like
  Conjugate Gradient or SMO show that GBCD is an order of magnitude faster. Comparisons
  against sparse GP methods show that GBCD is both accurate and capable of handling
  datasets of 100,000 samples or more.
title: Greedy block coordinate descent for large scale Gaussian process regression
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bo08a
month: 0
tex_title: Greedy block coordinate descent for large scale Gaussian process regression
firstpage: 43
lastpage: 52
page: 43-52
order: 43
cycles: false
bibtex_author: Bo, Liefeng and Sminchisescu, Cristian
author:
- given: Liefeng
  family: Bo
- given: Cristian
  family: Sminchisescu
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/bo08a/bo08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
