---
abstract: Bounded policy iteration is an approach to solving infinite-horizon POMDPs
  that represents policies as stochastic finite-state controllers and iteratively
  improves a controller by adjusting the parameters of each node using linear programming.
  In the original algorithm, the size of the linear programs, and thus the complexity
  of policy improvement, depends on the number of parameters of each node, which grows
  with the size of the controller. But in practice, the number of parameters of a
  node with non-zero values is often very small, and does not grow with the size of
  the controller. Based on this observation, we develop a version of bounded policy
  iteration that leverages the sparse structure of a stochastic finite-state controller.
  In each iteration, it improves a policy by the same amount as the original algorithm,
  but with much better scalability.
title: Sparse stochastic finite-state controllers for POMDPs
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hansen08a
month: 0
tex_title: Sparse stochastic finite-state controllers for POMDPs
firstpage: 256
lastpage: 263
page: 256-263
order: 256
cycles: false
bibtex_author: Hansen, Eric A.
author:
- given: Eric A.
  family: Hansen
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/hansen08a/hansen08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
