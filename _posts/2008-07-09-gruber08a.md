---
abstract: Latent topic models have been successfully applied as an unsupervised topic
  discovery technique in large document collections. With the proliferation of hypertext
  document collection such as the Internet, there has also been great interest in
  extending these approaches to hypertext [6, 9]. These approaches typically model
  links in an analogous fashion to how they model words - the document-link co-occurrence
  matrix is modeled in the same way that the document-word co-occurrence matrix is
  modeled in standard topic models.In this paper we present a probabilistic generative
  model for hypertext document collections that explicitly models the generation of
  links. Specifically, links from a word w to a document d depend directly on how
  frequent the topic of w is in d, in addition to the in-degree of d. We show how
  to perform EM learning on this model efficiently. By not modeling links as analogous
  to words, we end up using far fewer free parameters and obtain better link prediction
  results.
title: Latent topic models for hypertext
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gruber08a
month: 0
tex_title: Latent topic models for hypertext
firstpage: 230
lastpage: 239
page: 230-239
order: 230
cycles: false
bibtex_author: Gruber, Amit and Rosen-Zvi, Michal and Weiss, Yair
author:
- given: Amit
  family: Gruber
- given: Michal
  family: Rosen-Zvi
- given: Yair
  family: Weiss
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/gruber08a/gruber08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
