---
abstract: Continuous state spaces and stochastic, switching dynamics characterize
  a number of rich, real-world domains, such as robot navigation across varying terrain.
  We describe a reinforcement-learning algorithm for learning in these domains and
  prove for certain environments the algorithm is probably approximately correct with
  a sample complexity that scales polynomially with the state-space dimension. Unfortunately,
  no optimal planning techniques exist in general for such problems; instead we use
  fitted value iteration to solve the learned MDP, and include the error due to approximate
  planning in our bounds. Finally, we report an experiment using a robotic car driving
  over varying terrain to demonstrate that these dynamics representations adequately
  capture real-world dynamics and that our algorithm can be used to efficiently solve
  such problems.
title: 'CORL: a continuous-state offset-dynamics reinforcement learner'
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: brunskill08a
month: 0
tex_title: 'CORL: a continuous-state offset-dynamics reinforcement learner'
firstpage: 53
lastpage: 61
page: 53-61
order: 53
cycles: false
bibtex_author: Brunskill, Emma and Leffler, Bethany R. and Li, Lihong and Littman,
  Michael L. and Roy, Nicholas
author:
- given: Emma
  family: Brunskill
- given: Bethany R.
  family: Leffler
- given: Lihong
  family: Li
- given: Michael L.
  family: Littman
- given: Nicholas
  family: Roy
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/brunskill08a/brunskill08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
