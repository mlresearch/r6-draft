---
abstract: 'Graphical models are usually learned without regard to the cost of doing
  inference with them. As a result, even if a good model is learned, it may perform
  poorly at prediction, because it requires approximate inference. We propose an alternative:
  learning models with a score function that directly penalizes the cost of inference.
  Specifically, we learn arithmetic circuits with a penalty on the number of edges
  in the circuit (in which the cost of inference is linear). Our algorithm is equivalent
  to learning a Bayesian network with context-specific independence by greedily splitting
  conditional distributions, at each step scoring the candidates by compiling the
  resulting network into an arithmetic circuit, and using its size as the penalty.
  We show how this can be done efficiently, without compiling a circuit from scratch
  for each candidate. Experiments on several real-world domains show that our algorithm
  is able to learn tractable models with very large treewidth, and yields more accurate
  predictions than a standard context-specific Bayesian network learner, in far less
  time.'
title: Learning arithmetic circuits
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lowd08a
month: 0
tex_title: Learning arithmetic circuits
firstpage: 383
lastpage: 392
page: 383-392
order: 383
cycles: false
bibtex_author: Lowd, Daniel and Domingos, Pedro
author:
- given: Daniel
  family: Lowd
- given: Pedro
  family: Domingos
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/lowd08a/lowd08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
