---
abstract: 'Variational Bayesian inference and (collapsed) Gibbs sampling are the two
  important classes of inference algorithms for Bayesian networks. Both have their
  advantages and disadvantages: collapsed Gibbs sampling is unbiased but is also inefficient
  for large count values and requires averaging over many samples to reduce variance.
  On the other hand, variational Bayesian inference is efficient and accurate for
  large count values but suffers from bias for small counts. We propose a hybrid algorithm
  that combines the best of both worlds: it samples very small counts and applies
  variational updates to large counts. This hybridization is shown to significantly
  improve test-set perplexity relative to variational inference at no computational
  cost.'
title: Hybrid variational/gibbs collapsed inference in topic models
year: '2008'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: welling08a
month: 0
tex_title: Hybrid variational/gibbs collapsed inference in topic models
firstpage: 587
lastpage: 594
page: 587-594
order: 587
cycles: false
bibtex_author: Welling, Max and Teh, Yee Whye and Kappen, Bert
author:
- given: Max
  family: Welling
- given: Yee Whye
  family: Teh
- given: Bert
  family: Kappen
date: 2008-07-09
note: Reissued by PMLR on 09 October 2024.
address:
container-title: Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence
volume: R6
genre: inproceedings
issued:
  date-parts:
  - 2008
  - 7
  - 9
pdf: https://raw.githubusercontent.com/mlresearch/r6/main/assets/welling08a/welling08a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
